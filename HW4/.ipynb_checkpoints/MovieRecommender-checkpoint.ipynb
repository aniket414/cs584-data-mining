{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a7dcaaba-554c-47e3-b07c-73474fde21d1",
   "metadata": {},
   "source": [
    "- Import Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f90d835c-dc2a-48e6-8841-871fe746c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import scipy as sp\n",
    "from functools import reduce\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.stats\n",
    "import scipy.spatial\n",
    "from math import sqrt\n",
    "import math"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b528510f-1536-449a-858a-359542c76508",
   "metadata": {},
   "source": [
    "- Read Data files using Pandas: Additional Files[Actors, Directors, Genres, Tags] + Train + Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73921a52-ab72-41a1-bc8a-fc768ba36320",
   "metadata": {},
   "outputs": [],
   "source": [
    "movieActors = pd.read_table('./additional_files/movie_actors.dat', sep='\\t', header=0, error_bad_lines=False)\n",
    "movieDirectors = pd.read_table('./additional_files/movie_directors.dat', sep='\\t', header=0, error_bad_lines=False)\n",
    "movieGenres = pd.read_table('./additional_files/movie_genres.dat', sep='\\t', header=0, error_bad_lines=False)\n",
    "movieTags = pd.read_table('./additional_files/movie_tags.dat', sep='\\t', header=0, error_bad_lines=False)\n",
    "userTaggedMovies = pd.read_table('./additional_files/user_taggedmovies.dat', sep=' ', header=0, error_bad_lines=False)\n",
    "tags = pd.read_table('./additional_files/tags.dat', sep='\\t', header=0, error_bad_lines=False)\n",
    "test = pd.read_table('./additional_files/test.dat', sep=' ', header=0, error_bad_lines=False)\n",
    "train = pd.read_table('./additional_files/train.dat', sep=' ', header=0)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e62c247e-1b79-4ee1-b281-a53ba349baaa",
   "metadata": {},
   "source": [
    "- The arrange() function is used to arrange the columns of the multiple tables read above by joining them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66b17ce3-a8a3-4851-b30e-b0681fab385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange(table, by, joining, column):\n",
    "    field = table.groupby(by=by).apply(lambda x:[','.join(x[joining])])\n",
    "    fieldDF = pd.DataFrame(field, columns=[column])\n",
    "    fieldDF.reset_index(inplace=True)\n",
    "    return fieldDF"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f6440692-c7a9-4806-bc65-9cf1ddb9fbf7",
   "metadata": {},
   "source": [
    "- Arrange the tables by merging the columns based on 'movieID'\n",
    "- Perform this operation on each table by calling the arrange() function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65ca3d25-d009-4e6d-97fb-0d4e528c9e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topActors = movieActors.sort_values(['movieID', 'ranking'], ascending=True).groupby('movieID').head(3)\n",
    "actors = topActors.drop(['actorID', 'ranking'], axis=1)\n",
    "actorsDF = arrange(actors, 'movieID', 'actorName', 'actors')\n",
    "directorsDF = arrange(movieDirectors, 'movieID', 'directorName', 'directors')\n",
    "genreDF = arrange(movieGenres, 'movieID', 'genre', 'genres')\n",
    "\n",
    "movieTag = pd.merge(movieTags, tags, left_on=['tagID'], right_on=['id']).drop(['id'], axis=1)\n",
    "movieTagsDF = arrange(movieTag, 'movieID', 'value', 'usertags')\n",
    "\n",
    "userTaggedMovie = pd.merge(userTaggedMovies, tags, left_on=['tagID'], right_on=['id']).drop(['id'], axis=1)\n",
    "userTaggedMoviesDF = arrange(userTaggedMovie, 'movieID', 'value', 'tags')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b23ae468-7752-448d-bdab-6285ef3735aa",
   "metadata": {},
   "source": [
    "- Form a final movies table by merging Actors + Directors + Genre + Tags based on MovieId as arranged above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97354678-265a-4818-b44d-4530dd5f4d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = reduce(lambda x, y: pd.merge(x, y, on='movieID', how='outer'), [actorsDF, directorsDF, genreDF, userTaggedMoviesDF, movieTagsDF])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b6a3d21-4e8d-430e-9b8b-99caf5021aa3",
   "metadata": {},
   "source": [
    "- The cleanData() function is used replace spaces with empty value and convert all strings to lowercase\n",
    "- The mergeDetails() function performs a merge operation and joins all the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58f3d484-7525-4c2a-86a7-c7903f30e481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(x):\n",
    "    if isinstance(x, list):\n",
    "        return [str.lower(i.replace(' ', '')) for i in x]\n",
    "    else:\n",
    "        return str.lower(x.replace(' ', '')) if isinstance(x, str) else ''\n",
    "\n",
    "def mergeDetails(x):\n",
    "    return ' '.join(x['actors']) + ' ' + ' '.join(x['directors']) + ' ' +' '.join(x['genres']) + ' ' + ' '.join(x['tags'])+ ' ' + ' '.join(x['usertags'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f6caacf-ff0f-4790-8814-3e7c85af5492",
   "metadata": {},
   "source": [
    "- Clean each column of the movies table by calling the cleanData() function defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1df374f9-71d1-480a-bf25-35b7b961d0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['actors', 'directors', 'genres', 'tags', 'usertags']\n",
    "\n",
    "for column in columns:\n",
    "    movies[column] = movies[column].apply(cleanData)\n",
    "    \n",
    "movies['details'] = movies.apply(mergeDetails, axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5f325d9-5200-4242-8e9a-f3da4bb54271",
   "metadata": {},
   "source": [
    "- The predict() function is defined below to be used by both Content Based and Correlative Filtering method\n",
    "- The function decides rating based on similarity of 'userId' or 'movieId'\n",
    "- If the predict is called by Content Based method then rating is decided based on 'userId' else if it is called by Correlative Filtering then 'movieId' is used to decide the rating\n",
    "\n",
    "- The predict() function takes a boolean parameter isCF based on which it decides whether it is called by Content Based or Correlative Filtering method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58c21884-6197-4c74-82d7-51c9b837e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(userId, movieId, similarityValue, values, rate, isCF = True):\n",
    "    ratingList, weightList = [], []\n",
    "    predicted = 3.0\n",
    "    \n",
    "    for j, i in enumerate(similarityValue):\n",
    "        try:\n",
    "            if isCF == True: \n",
    "                rating = rate.loc[i, movieId]\n",
    "                similarity = values[j]\n",
    "            elif isCF == False:\n",
    "                rating = rate.loc[userId, i]\n",
    "                similarity = values[j]\n",
    "            if np.isnan(rating):\n",
    "                continue\n",
    "            elif not np.isnan(rating):\n",
    "                ratingList.append(rating * similarity)\n",
    "                weightList.append(similarity)\n",
    "        except KeyError:                                               \n",
    "            pass\n",
    "        \n",
    "    if sum(weightList) != 0:\n",
    "        predicted = round(sum(ratingList)/sum(weightList))\n",
    "        \n",
    "    if predicted < 0:\n",
    "        predicted = 0.0\n",
    "    elif predicted > 5:\n",
    "        predicted = 5.0\n",
    "        \n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f9565cd4-db9a-45ee-86dc-a1740338d2b5",
   "metadata": {},
   "source": [
    "- Content Based method\n",
    "\n",
    "- The tfidfVectorizer() is used to convert movie details to a sparse matrix of word representation\n",
    "- The sparse matrix is then used to find the inverse document frequency, it reduces the priority of frequently occurring words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04cb9557-494c-4022-a4b2-8ca5c58c7552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidfVectorizer(movies):\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    movies['details'] = movies['details'].fillna('')\n",
    "    return tfidf.fit_transform(movies['details'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e91087e-c20c-4e36-9bc5-347545cbda83",
   "metadata": {},
   "source": [
    "- The contentBased method is used to perform contentBased recommendation\n",
    "- Call the tfidfVectorizer() and then find the cosine similarity\n",
    "- Iterate the test dataframe and for each row do the following:\n",
    "    -> Get the index of the movie that matches the movieId\n",
    "    -> Get the pairwsie similarity scores of all movies similar to that movie\n",
    "    -> Sort the movies based on the similarity scores\n",
    "    -> Get the scores of the 'K' most similar movies\n",
    "    -> Get the movie indexes\n",
    "    -> Write the prediction in output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a03a957-69d7-4cf1-bc7b-eedccaa04912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contentBased(trainDF, testDF, movies, K):\n",
    "    rate = trainDF.pivot_table(index=['userID'], columns=['movieID'], values='rating')\n",
    "    tfidf = tfidfVectorizer(movies)\n",
    "    movieCosineSimilarity = cosine_similarity(tfidf, tfidf) \n",
    "    movies = movies.reset_index()\n",
    "    index = pd.Series(movies.index, index=movies['movieID'])\n",
    "    output = []\n",
    "    \n",
    "    for i, r in tqdm(testDF.iterrows()):\n",
    "        userId = testDF.loc[i]['userID']\n",
    "        movieId = testDF.loc[i]['movieID']\n",
    "        similarityScores = list(enumerate(movieCosineSimilarity[index[movieId]]))\n",
    "        similarityScores = sorted(similarityScores, key=lambda x: x[1], reverse=True)\n",
    "        similarityScores = similarityScores[1: K]\n",
    "        similarityValues = [i[1] for i in similarityScores]\n",
    "        similarityMovies = [i[0] for i in similarityScores]\n",
    "        predicted = predict(userId, movieId, similarityMovies, similarityValues, rate, isCF=False)\n",
    "        output.append(predicted)\n",
    "        \n",
    "    with open('cb.txt', 'w') as file:\n",
    "        for rating in output:\n",
    "            file.write(str((int(rating))))\n",
    "            file.write('\\n')\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c714332b-715d-4911-a15d-bf5a36d429cc",
   "metadata": {},
   "source": [
    "- Correlative Filtering method\n",
    "\n",
    "- The getSimilarity() is used to find the cosine similarity and pearson corelation similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "073251e1-83df-472a-8879-971927dc63c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimilarity(normalizedRate, sparseRate, normalizedUsers):\n",
    "    userCosineSimilarity = np.zeros((normalizedUsers, normalizedUsers))\n",
    "    userCosineSimilarity = cosine_similarity(sparseRate.T)\n",
    "    userPearsonCorelationSimilarity = np.zeros((normalizedUsers, normalizedUsers))\n",
    "    \n",
    "    for i in tqdm(range(normalizedUsers)):\n",
    "        for j in range(normalizedUsers):\n",
    "            if np.count_nonzero(normalizedRate.iloc[i, :]) and np.count_nonzero(normalizedRate.iloc[j, :]):\n",
    "                try:\n",
    "                    if not math.isnan(scipy.stats.pearsonr(normalizedRate.iloc[i, :], normalizedRate.iloc[j, :])[0]):\n",
    "                        userPearsonCorelationSimilarity[i][j] = scipy.stats.pearsonr(normalizedRate.iloc[i, :], normalizedRate.iloc[j, :])[0]\n",
    "                    else:\n",
    "                        userPearsonCorelationSimilarity[i][j] = 0\n",
    "                except:\n",
    "                    userPearsonCorelationSimilarity[i][j] = 0\n",
    "                    \n",
    "    return userCosineSimilarity, userPearsonCorelationSimilarity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "11e3f96c-a030-4ced-b9cf-0a8957d02b53",
   "metadata": {},
   "source": [
    "- The preprocess() function is used to normalize the ratings of the movie before applying Correlative Filtering method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1724a9e1-3a20-40bd-aa35-aea3d0a0f051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(rate):\n",
    "    normalizedRate = rate.apply(lambda x: (x-np.mean(x))/(np.max(x)-np.min(x)), axis=1)\n",
    "    normalizedRate.fillna(0, inplace=True)\n",
    "    normalizedRate = normalizedRate.T\n",
    "    sparseRate = sp.sparse.csr_matrix(normalizedRate.values)\n",
    "    return normalizedRate, sparseRate"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e280c52-a180-4612-a1bc-988d28ff55cd",
   "metadata": {},
   "source": [
    "- The correlativeFiltering method is used to perform correlativeFiltering recommendation\n",
    "- Normalize the movie ratings by calling the preprocess() function\n",
    "- Get the cosine similarity and pearson correlation similarity\n",
    "- Iterate the test dataframe and for each row do the following:\n",
    "    -> Get nearest best neighbors\n",
    "    -> Get the user indexes\n",
    "    -> Get the scores of the 'K' most similar users\n",
    "    -> Write the prediction in output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f45e7b0f-b376-45fd-897f-2bdd2b9436c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlativeFiltering(trainDF, testDF, k):\n",
    "    \n",
    "    normalizedUsers = trainDF.userID.unique().shape[0]\n",
    "    rate = trainDF.pivot_table(index=['userID'], columns=['movieID'], values='rating')\n",
    "    normalizedRate, sparseRate = preprocess(rate)\n",
    "    userCosineSimilarity, userPearsonCorelationSimilarity = getSimilarity(normalizedRate, sparseRate, normalizedUsers)\n",
    "    userSimilarityDF = pd.DataFrame(userCosineSimilarity, index=normalizedRate.columns, columns=normalizedRate.columns)\n",
    "    #userSimilarityDF = pd.DataFrame(userPearsonCorelationSimilarity, index=normalizedRate.columns, columns=normalizedRate.columns)\n",
    "    output = []\n",
    "    \n",
    "    for i, r in testDF.iterrows():\n",
    "        userId = testDF.loc[i]['userID']\n",
    "        movieId = testDF.loc[i]['movieID']\n",
    "        userSimilarity = userSimilarityDF.sort_values(by=userId, ascending=False).index[1: k]\n",
    "        similarityValues = userSimilarityDF.sort_values(by=userId, ascending=False).loc[:, userId].tolist()[1: k]\n",
    "        predicted = predict(userId, movieId, userSimilarity, similarityValues, rate)\n",
    "        output.append(predicted)\n",
    "        \n",
    "    with open('cf.txt', 'w') as file:\n",
    "        for rating in output:\n",
    "            file.write(str((int(rating))))\n",
    "            file.write('\\n')\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b25415e9-aec1-4d20-a3f8-0bf073f9846f",
   "metadata": {},
   "source": [
    "- The crossValidation() function is used to perform 10 fold cross validation for correlative filtering and content based method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd35ac50-873a-4ffe-b5d0-75da933283a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidation(train, movies, isCF = True):\n",
    "    rmse, actual = 0.0, []\n",
    "    rating = train.loc[:, ['rating']]\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=None)\n",
    "    \n",
    "    for k in (50, 500, 1500):\n",
    "        for trainIndex, testIndex in kf.split(train):\n",
    "            trainCV = train.loc[trainIndex, :]\n",
    "            testCV = train.loc[testIndex, :].drop('rating', axis=1)\n",
    "            actual = rating.iloc[testIndex].values\n",
    "            \n",
    "            if isCF == True:\n",
    "                predict = correlativeFiltering(trainCV, testCV, k)\n",
    "            elif isCF == False:\n",
    "                predict = contentBased(trainCV, testCV, movies, k)\n",
    "                \n",
    "            rmse += sqrt(mean_squared_error(predict, actual))\n",
    "            \n",
    "        print(\"RMSE for K = \" + str(k) + \" is \" + str(rmse/10))\n",
    "        rmse = 0.0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "92f5e886-7688-4c79-963a-903599b9d938",
   "metadata": {},
   "source": [
    "- Perform crossValidation() for correlative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914c758d-9065-40d1-8ea7-2f94b09e1535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2113/2113 [33:19<00:00,  1.06it/s]\n",
      "100%|██████████| 2113/2113 [33:59<00:00,  1.04it/s]\n",
      "100%|██████████| 2113/2113 [34:21<00:00,  1.03it/s]\n",
      "100%|██████████| 2113/2113 [33:24<00:00,  1.05it/s]\n",
      " 64%|██████▍   | 1360/2113 [21:44<12:02,  1.04it/s]"
     ]
    }
   ],
   "source": [
    "crossValidation(train, movies, isCF=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "790e4270-ee78-4a8e-a189-376f9d1eb484",
   "metadata": {},
   "source": [
    "- Perform crossValidation() for content based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae082df-3f1b-4142-98e7-11a248be7bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64170it [05:52, 182.29it/s]\n",
      "64170it [05:52, 181.99it/s]\n",
      "64170it [05:40, 188.23it/s]\n",
      "64170it [05:59, 178.70it/s]\n",
      "64170it [05:53, 181.66it/s]\n",
      "64170it [05:53, 181.77it/s]\n",
      "64170it [05:50, 183.20it/s]\n",
      "64170it [05:56, 179.97it/s]\n",
      "64170it [05:38, 189.55it/s]\n",
      "64169it [05:43, 186.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for K = 50 is 1.1675861255291846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64170it [15:38, 68.35it/s]\n",
      "64170it [15:59, 66.88it/s]\n",
      "64170it [16:13, 65.94it/s]\n",
      "64170it [15:33, 68.73it/s]\n",
      "64170it [15:34, 68.66it/s]\n",
      "64170it [15:24, 69.38it/s]\n",
      "64170it [15:20, 69.72it/s]\n",
      "64170it [15:34, 68.69it/s]\n",
      "64170it [15:18, 69.90it/s]\n",
      "64169it [15:21, 69.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for K = 500 is 1.0176048287159696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64170it [37:06, 28.82it/s]\n",
      "64170it [37:05, 28.83it/s]\n",
      "64170it [36:51, 29.02it/s]\n",
      "16786it [10:00, 27.12it/s]"
     ]
    }
   ],
   "source": [
    "crossValidation(train, movies, isCF=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f61e131a-b9d5-48ee-a9ca-b87083707dd2",
   "metadata": {},
   "source": [
    "Find the actual prediction of rating using K = 500 for correlative filtering method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65581b21-cf71-4037-8983-14cb47364cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2113/2113 [33:36<00:00,  1.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3.0,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3.0,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3.0,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3.0,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlativeFiltering(train, test, 500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
